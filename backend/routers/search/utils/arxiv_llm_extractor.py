"""LLM-powered extraction of training dataset information from arxiv papers."""

import os
import json
import logging
from typing import List, Optional
from dataclasses import dataclass
from openai import OpenAI

logger = logging.getLogger(__name__)


@dataclass
class ExtractedDataset:
    """Structured dataset information extracted by LLM."""

    name: str
    type: str  # "public_dataset", "synthetic", "proprietary", "mixture"
    source: Optional[str] = None  # Model name if synthetic, URL if public
    context: Optional[str] = None  # Brief description of how it was used
    hf_url: Optional[str] = None  # HuggingFace URL if available


class LLMDatasetExtractor:
    """Extract structured dataset information using LLM."""

    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not found, LLM extraction will be disabled")
            self.client = None
        else:
            self.client = OpenAI(api_key=api_key)

    def extract_datasets(
        self, paper_text: str, model_id: str, arxiv_url: str
    ) -> List[ExtractedDataset]:
        """
        Use LLM to extract structured dataset information from paper text.

        Args:
            paper_text: Text extracted from the arxiv paper (first 8 pages)
            model_id: HuggingFace model ID being analyzed
            arxiv_url: URL to the arxiv paper

        Returns:
            List of ExtractedDataset objects with structured information
        """
        if not self.client:
            logger.warning("LLM client not initialized, falling back to empty results")
            return []

        # Limit text to avoid token limits (roughly 10k chars = ~2.5k tokens)
        paper_excerpt = paper_text[:10000]

        prompt = f"""You are analyzing an AI research paper to extract training dataset information.

**Paper excerpt:**
{paper_excerpt}

**Model being analyzed:** {model_id}
**Paper URL:** {arxiv_url}

Extract ALL datasets mentioned in the training, pretraining, or fine-tuning sections. For each dataset found:

1. **name**: Specific dataset name. Examples:
   - "BookCorpus" (public dataset)
   - "Synthetic data from Qwen2.5-Math" (synthetic data)
   - "Wikipedia English" (specific variant)
   - "Custom internal dataset" (proprietary)

2. **type**: One of:
   - "public_dataset": Publicly available (e.g., SQuAD, ImageNet)
   - "synthetic": Generated by another model (e.g., "data from GPT-4")
   - "proprietary": Custom/internal dataset
   - "mixture": Combination of multiple sources

3. **source**:
   - For synthetic data: Source model name/ID (e.g., "Qwen/Qwen2.5-Math")
   - For public datasets: null or dataset identifier
   - For proprietary: Organization name if mentioned

4. **context**: Brief 1-sentence description of how it was used

5. **hf_url**: HuggingFace dataset URL if you can infer it from the name
   - Format: "https://huggingface.co/datasets/{{dataset_id}}"
   - Examples:
     * "squad" → "https://huggingface.co/datasets/rajpurkar/squad"
     * "imagenet" → "https://huggingface.co/datasets/imagenet-1k"
     * "bookcorpus" → "https://huggingface.co/datasets/bookcorpus"
   - Leave as null if unsure

IMPORTANT:
- Be specific: Don't just extract "dataset", extract actual names
- For synthetic data, include the source model in the name
- Extract datasets even if mentioned briefly
- Ignore validation/test sets, focus on training data

Return valid JSON only (no markdown, no explanation):
{{
  "datasets": [
    {{
      "name": "BookCorpus",
      "type": "public_dataset",
      "source": null,
      "context": "Used for pretraining alongside Wikipedia",
      "hf_url": "https://huggingface.co/datasets/bookcorpus"
    }},
    {{
      "name": "Synthetic data from Qwen2.5-Math",
      "type": "synthetic",
      "source": "Qwen/Qwen2.5-Math",
      "context": "Generated for mathematical reasoning tasks",
      "hf_url": null
    }}
  ]
}}
"""

        try:
            logger.info(f"Calling LLM to extract datasets from {arxiv_url}")

            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # Fast and cheap
                messages=[
                    {
                        "role": "system",
                        "content": "You extract structured dataset information from research papers. Always return valid JSON only.",
                    },
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=1500,
            )

            content = response.choices[0].message.content
            if not content:
                logger.warning("Empty response from LLM")
                return []

            data = json.loads(content)
            datasets = data.get("datasets", [])

            logger.info(f"LLM extracted {len(datasets)} datasets from {arxiv_url}")

            # Convert to ExtractedDataset objects
            result = []
            for ds in datasets:
                try:
                    result.append(
                        ExtractedDataset(
                            name=ds.get("name", "Unknown"),
                            type=ds.get("type", "public_dataset"),
                            source=ds.get("source"),
                            context=ds.get("context"),
                            hf_url=ds.get("hf_url"),
                        )
                    )
                except Exception as e:
                    logger.warning(f"Error parsing dataset entry: {e}")
                    continue

            return result

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM JSON response: {e}")
            logger.error(f"Response content: {content}")
            return []
        except Exception as e:
            logger.error(f"Error calling LLM for dataset extraction: {e}")
            return []

    def is_available(self) -> bool:
        """Check if LLM extraction is available."""
        return self.client is not None
